# CoT Implementation w/ multi-modal inputs

<img width="1107" alt="Screenshot 2025-02-26 at 8 39 08â€¯PM" src="https://github.com/user-attachments/assets/df4369d8-7841-4a18-9124-fb2ca7d5d925" />

## Overview

This technical interview focuses on implementing Chain of Thought (CoT) reasoning capabilities with support for multi-modal inputs including images and PDFs. The system leverages CoT prompting to break down complex reasoning tasks into intermediate steps, enhancing the model's ability to process and analyze uploaded documents and images. More details found here [here](https://docs.google.com/document/d/18yYc0IseR5TxL3PV9asff4RjcYvwrf2eD2Af81Qg7fs/edit?usp=sharing)

## Prerequisites

- Node.js (v14+)
- npm (v6+)

## Installation

1. **Clone the Repository:**

   ```
   git clone https://github.com/UniversalAGI/tech-interview-multi-modal-cot
   cd tech-interview-multi-modal-cot
   ```

2. **Install Dependencies:**

   ```
   cd web
   npm install
   ```

## Running the Next.js Frontend

1. **Start the Development Server:**

   ```
   npm run dev
   ```

   This will launch the Next.js application in development mode. Open [http://localhost:3000](http://localhost:3000) to view it in the browser.
